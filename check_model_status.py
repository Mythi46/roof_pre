#!/usr/bin/env python3
"""
å…¨é¢æ£€æŸ¥æ¨¡å‹çŠ¶æ€å’Œå¯ç”¨èµ„æº
Comprehensive model status and resource check
"""

import os
import sys
import yaml
from pathlib import Path
import json
from datetime import datetime

def check_project_structure():
    """æ£€æŸ¥é¡¹ç›®ç»“æ„"""
    print("ğŸ” Checking Project Structure")
    print("=" * 50)
    
    # å…³é”®ç›®å½•æ£€æŸ¥
    key_dirs = [
        "data/raw/new-2-1",
        "runs/segment",
        "models",
        "visualization_results",
        "src"
    ]
    
    for dir_path in key_dirs:
        if os.path.exists(dir_path):
            print(f"âœ… {dir_path} - EXISTS")
            # æ˜¾ç¤ºå­ç›®å½•
            try:
                subdirs = [d for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]
                if subdirs:
                    print(f"   ğŸ“ Subdirectories: {', '.join(subdirs[:5])}")
            except:
                pass
        else:
            print(f"âŒ {dir_path} - MISSING")
    
    print()

def check_trained_models():
    """æ£€æŸ¥è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("ğŸ¤– Checking Trained Models")
    print("=" * 50)
    
    # å¯èƒ½çš„æ¨¡å‹ä½ç½®
    model_locations = [
        "runs/segment/improved_training_compatible/weights/best.pt",
        "runs/segment/continue_training_optimized/weights/best.pt",
        "runs/segment/*/weights/best.pt",
        "models/trained/best.pt",
        "best.pt"
    ]
    
    found_models = []
    
    for location in model_locations:
        if '*' in location:
            # å¤„ç†é€šé…ç¬¦è·¯å¾„
            base_path = location.split('*')[0]
            if os.path.exists(base_path):
                try:
                    for subdir in os.listdir(base_path):
                        full_path = os.path.join(base_path, subdir, "weights", "best.pt")
                        if os.path.exists(full_path):
                            found_models.append(full_path)
                except:
                    pass
        else:
            if os.path.exists(location):
                found_models.append(location)
    
    if found_models:
        print(f"âœ… Found {len(found_models)} trained models:")
        for i, model in enumerate(found_models, 1):
            file_size = os.path.getsize(model) / (1024*1024)  # MB
            mod_time = datetime.fromtimestamp(os.path.getmtime(model))
            print(f"   {i}. {model}")
            print(f"      Size: {file_size:.1f} MB")
            print(f"      Modified: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}")
    else:
        print("âŒ No trained models found")
    
    print()
    return found_models

def check_dataset():
    """æ£€æŸ¥æ•°æ®é›†"""
    print("ğŸ“Š Checking Dataset")
    print("=" * 50)
    
    data_yaml_path = "data/raw/new-2-1/data.yaml"
    
    if not os.path.exists(data_yaml_path):
        print(f"âŒ Dataset config not found: {data_yaml_path}")
        return None
    
    print(f"âœ… Dataset config found: {data_yaml_path}")
    
    try:
        with open(data_yaml_path, 'r') as f:
            data_config = yaml.safe_load(f)
        
        print(f"ğŸ“‹ Classes: {data_config.get('nc', 'Unknown')} classes")
        print(f"   Names: {data_config.get('names', [])}")
        
        # æ£€æŸ¥å›¾ç‰‡ç›®å½•
        dataset_root = Path(data_yaml_path).parent
        
        for split in ['train', 'val', 'test']:
            if split in data_config:
                rel_path = data_config[split]
                if rel_path.startswith('../'):
                    abs_path = dataset_root / rel_path
                else:
                    abs_path = Path(rel_path)
                
                if abs_path.exists():
                    try:
                        image_files = [f for f in abs_path.iterdir() 
                                     if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]
                        print(f"   âœ… {split}: {len(image_files)} images in {abs_path}")
                    except Exception as e:
                        print(f"   âŒ {split}: Error reading {abs_path} - {e}")
                else:
                    print(f"   âŒ {split}: Directory not found - {abs_path}")
        
        return data_config
        
    except Exception as e:
        print(f"âŒ Error reading dataset config: {e}")
        return None
    
    print()

def check_visualization_results():
    """æ£€æŸ¥å¯è§†åŒ–ç»“æœ"""
    print("ğŸ¨ Checking Visualization Results")
    print("=" * 50)
    
    viz_dirs = [
        "visualization_results",
        "visualization_results_50"
    ]
    
    for viz_dir in viz_dirs:
        if os.path.exists(viz_dir):
            try:
                files = os.listdir(viz_dir)
                png_files = [f for f in files if f.endswith('.png')]
                jpg_files = [f for f in files if f.endswith('.jpg')]
                html_files = [f for f in files if f.endswith('.html')]
                json_files = [f for f in files if f.endswith('.json')]
                
                print(f"âœ… {viz_dir}:")
                print(f"   ğŸ“Š PNG charts: {len(png_files)}")
                print(f"   ğŸ–¼ï¸ JPG images: {len(jpg_files)}")
                print(f"   ğŸŒ HTML files: {len(html_files)}")
                print(f"   ğŸ“‹ JSON files: {len(json_files)}")
                
                # æ˜¾ç¤ºä¸€äº›å…³é”®æ–‡ä»¶
                key_files = ['index.html', 'detection_summary.png', 'detection_results.json']
                for key_file in key_files:
                    if key_file in files:
                        print(f"   âœ… {key_file}")
                    else:
                        print(f"   âŒ {key_file} missing")
                        
            except Exception as e:
                print(f"âŒ Error reading {viz_dir}: {e}")
        else:
            print(f"âŒ {viz_dir} not found")
    
    print()

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åº“"""
    print("ğŸ“¦ Checking Dependencies")
    print("=" * 50)
    
    required_packages = [
        'ultralytics',
        'opencv-python',
        'matplotlib',
        'numpy',
        'pillow',
        'yaml',
        'torch'
    ]
    
    for package in required_packages:
        try:
            if package == 'opencv-python':
                import cv2
                version = cv2.__version__
            elif package == 'yaml':
                import yaml
                version = getattr(yaml, '__version__', 'Unknown')
            else:
                module = __import__(package)
                version = getattr(module, '__version__', 'Unknown')
            
            print(f"âœ… {package}: {version}")
            
        except ImportError:
            print(f"âŒ {package}: NOT INSTALLED")
        except Exception as e:
            print(f"âš ï¸ {package}: Error - {e}")
    
    print()

def check_training_results():
    """æ£€æŸ¥è®­ç»ƒç»“æœ"""
    print("ğŸ“ˆ Checking Training Results")
    print("=" * 50)
    
    training_dirs = []
    
    # æŸ¥æ‰¾è®­ç»ƒç»“æœç›®å½•
    if os.path.exists("runs/segment"):
        for subdir in os.listdir("runs/segment"):
            full_path = os.path.join("runs/segment", subdir)
            if os.path.isdir(full_path):
                training_dirs.append(full_path)
    
    if training_dirs:
        print(f"âœ… Found {len(training_dirs)} training runs:")
        
        for train_dir in training_dirs:
            print(f"\nğŸ“ {train_dir}:")
            
            # æ£€æŸ¥å…³é”®æ–‡ä»¶
            key_files = {
                'results.csv': 'è®­ç»ƒç»“æœ',
                'args.yaml': 'è®­ç»ƒå‚æ•°',
                'weights/best.pt': 'æœ€ä½³æ¨¡å‹',
                'weights/last.pt': 'æœ€åæ¨¡å‹'
            }
            
            for file_path, description in key_files.items():
                full_file_path = os.path.join(train_dir, file_path)
                if os.path.exists(full_file_path):
                    if file_path.endswith('.pt'):
                        size_mb = os.path.getsize(full_file_path) / (1024*1024)
                        print(f"   âœ… {description}: {size_mb:.1f} MB")
                    else:
                        print(f"   âœ… {description}")
                else:
                    print(f"   âŒ {description} missing")
            
            # è¯»å–è®­ç»ƒç»“æœ
            results_csv = os.path.join(train_dir, 'results.csv')
            if os.path.exists(results_csv):
                try:
                    import pandas as pd
                    df = pd.read_csv(results_csv)
                    if len(df) > 0:
                        last_row = df.iloc[-1]
                        print(f"   ğŸ“Š Final epoch: {len(df)}")
                        if 'metrics/mAP50(B)' in df.columns:
                            print(f"   ğŸ¯ Final mAP@0.5: {last_row['metrics/mAP50(B)']:.3f}")
                        if 'metrics/mAP50-95(B)' in df.columns:
                            print(f"   ğŸ¯ Final mAP@0.5:0.95: {last_row['metrics/mAP50-95(B)']:.3f}")
                except Exception as e:
                    print(f"   âš ï¸ Error reading results: {e}")
    else:
        print("âŒ No training results found")
    
    print()

def generate_status_report():
    """ç”ŸæˆçŠ¶æ€æŠ¥å‘Š"""
    print("ğŸ“‹ Generating Status Report")
    print("=" * 50)
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'project_status': 'checking...',
        'summary': {}
    }
    
    # æ£€æŸ¥å„ä¸ªç»„ä»¶
    models = check_trained_models()
    dataset = check_dataset()
    
    # ç”Ÿæˆæ€»ç»“
    report['summary'] = {
        'trained_models': len(models) if models else 0,
        'dataset_available': dataset is not None,
        'visualization_exists': os.path.exists('visualization_results'),
        'ready_for_50_images': len(models) > 0 and dataset is not None
    }
    
    if report['summary']['ready_for_50_images']:
        report['project_status'] = 'READY'
        print("ğŸ‰ Project Status: READY for 50 images visualization")
        print("ğŸ’¡ Recommended next steps:")
        print("   1. Run 50 images visualization")
        print("   2. Generate extended analysis")
        print("   3. Create comprehensive report")
    else:
        report['project_status'] = 'NOT_READY'
        print("âš ï¸ Project Status: NOT READY")
        print("ğŸ”§ Required actions:")
        if not models:
            print("   - Train or locate model files")
        if not dataset:
            print("   - Fix dataset configuration")
    
    # ä¿å­˜æŠ¥å‘Š
    with open('project_status_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"\nğŸ“„ Status report saved: project_status_report.json")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Comprehensive Model and Project Status Check")
    print("=" * 60)
    print(f"ğŸ“… Check Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # æ‰§è¡Œå„é¡¹æ£€æŸ¥
    check_project_structure()
    check_dependencies()
    models = check_trained_models()
    dataset = check_dataset()
    check_training_results()
    check_visualization_results()
    generate_status_report()
    
    print("ğŸ Status check completed!")

if __name__ == "__main__":
    main()

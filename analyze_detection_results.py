#!/usr/bin/env python3
"""
åˆ†æžæ£€æµ‹ç»“æžœç»Ÿè®¡
"""

import json
import numpy as np
from collections import Counter

def analyze_detection_results():
    """åˆ†æžæ£€æµ‹ç»“æžœ"""
    
    # è¯»å–ç»“æžœæ–‡ä»¶
    with open('visualization_results/detection_results.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("ðŸŽ¨ å±‹é¡¶æ£€æµ‹å¯è§†åŒ–ç»“æžœåˆ†æž")
    print("=" * 50)
    
    # åŸºæœ¬ç»Ÿè®¡
    total_images = data['total_images']
    total_detections = data['total_detections']
    
    print(f"ðŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"   æ€»å›¾ç‰‡æ•°: {total_images}")
    print(f"   æ€»æ£€æµ‹æ•°: {total_detections}")
    print(f"   å¹³å‡æ¯å›¾: {total_detections/total_images:.1f}ä¸ªç›®æ ‡")
    
    # æ¯å¼ å›¾ç‰‡çš„æ£€æµ‹æ•°é‡
    detection_counts = [result['num_detections'] for result in data['results']]
    
    print(f"\nðŸ“ˆ æ£€æµ‹æ•°é‡åˆ†å¸ƒ:")
    print(f"   æœ€å¤šæ£€æµ‹: {max(detection_counts)}ä¸ªç›®æ ‡")
    print(f"   æœ€å°‘æ£€æµ‹: {min(detection_counts)}ä¸ªç›®æ ‡")
    print(f"   ä¸­ä½æ•°: {np.median(detection_counts):.1f}ä¸ªç›®æ ‡")
    print(f"   æ ‡å‡†å·®: {np.std(detection_counts):.1f}")
    
    # ç±»åˆ«ç»Ÿè®¡
    all_classes = []
    all_confidences = []
    
    for result in data['results']:
        for detection in result['detections']:
            all_classes.append(detection['class'])
            all_confidences.append(detection['confidence'])
    
    class_counts = Counter(all_classes)
    
    print(f"\nðŸ·ï¸ ç±»åˆ«æ£€æµ‹ç»Ÿè®¡:")
    for class_name, count in class_counts.most_common():
        percentage = (count / total_detections) * 100
        print(f"   {class_name}: {count}ä¸ª ({percentage:.1f}%)")
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    if all_confidences:
        print(f"\nðŸŽ¯ ç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(all_confidences):.3f}")
        print(f"   æœ€é«˜ç½®ä¿¡åº¦: {np.max(all_confidences):.3f}")
        print(f"   æœ€ä½Žç½®ä¿¡åº¦: {np.min(all_confidences):.3f}")
        print(f"   ç½®ä¿¡åº¦æ ‡å‡†å·®: {np.std(all_confidences):.3f}")
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        high_conf = sum(1 for c in all_confidences if c >= 0.8)
        medium_conf = sum(1 for c in all_confidences if 0.5 <= c < 0.8)
        low_conf = sum(1 for c in all_confidences if c < 0.5)
        
        print(f"\nðŸ“Š ç½®ä¿¡åº¦åˆ†å¸ƒ:")
        print(f"   é«˜ç½®ä¿¡åº¦ (â‰¥0.8): {high_conf}ä¸ª ({high_conf/len(all_confidences)*100:.1f}%)")
        print(f"   ä¸­ç½®ä¿¡åº¦ (0.5-0.8): {medium_conf}ä¸ª ({medium_conf/len(all_confidences)*100:.1f}%)")
        print(f"   ä½Žç½®ä¿¡åº¦ (<0.5): {low_conf}ä¸ª ({low_conf/len(all_confidences)*100:.1f}%)")
    
    # å›¾ç‰‡è¯¦ç»†ä¿¡æ¯
    print(f"\nðŸ–¼ï¸ å›¾ç‰‡æ£€æµ‹è¯¦æƒ…:")
    for i, result in enumerate(data['results'], 1):
        image_name = result['image_path'].split('\\')[-1]
        num_det = result['num_detections']
        
        # ç»Ÿè®¡è¯¥å›¾ç‰‡çš„ç±»åˆ«
        image_classes = [det['class'] for det in result['detections']]
        image_class_counts = Counter(image_classes)
        class_summary = ', '.join([f"{cls}({cnt})" for cls, cnt in image_class_counts.items()])
        
        print(f"   å›¾ç‰‡{i:2d}: {image_name[:30]:<30} | {num_det:2d}ä¸ªç›®æ ‡ | {class_summary}")
    
    print(f"\nâœ… åˆ†æžå®Œæˆ!")
    print(f"ðŸ“ å¯è§†åŒ–ç»“æžœä¿å­˜åœ¨: visualization_results/")
    print(f"ðŸŒ æŸ¥çœ‹HTMLç”»å»Š: visualization_results/results_gallery.html")

if __name__ == "__main__":
    analyze_detection_results()
